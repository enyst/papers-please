#AI-papers 

Analyzing the Performance of Large Language Models on Code
Summarization
https://aclanthology.org/2024.lrec-main.89.pdf

Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
https://arxiv.org/pdf/2409.12640

NLPerturbator: Studying the Robustness of Code LLMs to Natural Language Variations
https://arxiv.org/pdf/2406.19783v1

Knowing When to Ask - Bridging Large Language Models and Data
https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf

Fact, Fetch, and Reason: A Unified Evaluation of RAG
https://arxiv.org/abs/2409.12941

HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly
https://arxiv.org/abs/2410.02694

LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations
https://arxiv.org/abs/2410.02707

Why think step by step? Reasoning emerges from the locality of experience
https://arxiv.org/abs/2304.03843

Lost in the Middle: How Language Models Use Long Contexts
https://arxiv.org/abs/2307.03172

Promptagator: Few-shot Dense Retrieval From 8 Examples
https://arxiv.org/abs/2209.11755

LLMS STILL CAN’T PLAN; CAN LRMS?
A PRELIMINARY EVALUATION OF OPENAI’S O1 ON PLANBENCH
https://arxiv.org/pdf/2409.13373

Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts
https://arxiv.org/abs/2409.13449

Let’s Verify Step by Step
https://arxiv.org/pdf/2305.20050

On the Limitations of Compute Thresholds as a Governance Strategy.
https://arxiv.org/pdf/2407.05694

Knowing When to Ask - Bridging Large Language Models and Data
https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf

MLE-BENCH: EVALUATING MACHINE LEARNING AGENTS ON MACHINE LEARNING ENGINEERING
https://arxiv.org/pdf/2410.07095

IMPLICIT SEARCH VIA DISCRETE DIFFUSION: A STUDY ON CHESS
- anonymous authors
- paper under double-blind review
https://openreview.net/pdf?id=A9y3LFX4ds

## Prompts

(refs from Embodied LLM Agents Learn to Cooperate in Organized Teams)

Language models are sensitive to prompts. The format of the prompt can have a substantial influence on performance [11, 59, 72, 46, 75, 41]. Various research efforts have aimed at prompt optimization. Typical approaches include heuristic search using language models’ knowledge [11, 47], first-order methods like soft prompt tuning [22], and prefix tuning [25]. In this work, we focus on obtaining an interpretable prompt in the form of natural language, drawing on insights from Yang et al. [62], Zhou et al. [73], and Pryzant et al. [38].



---

Refs from: https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization
- [G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment](https://arxiv.org/pdf/2303.16634.pdf) - Liu Y, Iter D, Xu Y, Wang S, Xu R, Zhu C. Published May, 2023.
- [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) - Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. Published online February, 2020.
- [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/) - Lin CY. Published July, 2004.
- [SummEval: Re-evaluating Summarization Evaluation](https://aclanthology.org/2021.tacl-1.24) - Fabbri et al. Published April, 2021

---
The Platonic Representation Hypothesis
https://arxiv.org/abs/2405.07987

(2013) Playing Atari with Deep Learning Reinforcement
https://paperswithcode.com/paper/playing-atari-with-deep-reinforcement
- origin of o1 / Q-learning

Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents
https://arxiv.org/abs/2311.13373
---

- Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
https://arxiv.org/pdf/2402.14207
https://x.com/lateinteraction/status/1840445257655464318?s=61

---

- Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models
	- https://arxiv.org/abs/2410.01280
- The difficulties of executing simple algorithms: Why brains make mistakes computers don’t
	- http://sapir.psych.wisc.edu/papers/lupyan_2013.pdf
- 